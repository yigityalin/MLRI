{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "root_dir = Path().resolve().parent.parent.parent.as_posix()\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "del root_dir\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet_v2\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import config, datasets, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'efficientnetv2b0-3_layer_dropout'\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6400 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = datasets.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(),\n",
    "    layers.RandomRotation(factor=0.25),\n",
    "    layers.RandomContrast(factor=0.2),\n",
    "    layers.RandomBrightness(factor=0.2),\n",
    "    models.RandomSaturation(2, 16),\n",
    "    models.RandomHue(0.4),\n",
    "    layers.GaussianNoise(stddev=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = keras.Sequential([\n",
    "    layers.Dense(2048, activation=keras.activations.relu),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(2048, activation=keras.activations.relu),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(2048, activation=keras.activations.relu),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(4, activation=keras.activations.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.create_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    preprocessing_layers=data_augmentation,\n",
    "    base_model=efficientnet_v2.EfficientNetV2B0,\n",
    "    top_layers=top,\n",
    "    pooling='avg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "160/160 [==============================] - 67s 326ms/step - loss: 1.7464 - sparse_categorical_accuracy: 0.4809 - val_loss: 0.9818 - val_sparse_categorical_accuracy: 0.5188\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 50s 311ms/step - loss: 1.0081 - sparse_categorical_accuracy: 0.5129 - val_loss: 0.9540 - val_sparse_categorical_accuracy: 0.5453\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9827 - sparse_categorical_accuracy: 0.5170 - val_loss: 0.9748 - val_sparse_categorical_accuracy: 0.5453\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 50s 311ms/step - loss: 0.9949 - sparse_categorical_accuracy: 0.5201 - val_loss: 0.9862 - val_sparse_categorical_accuracy: 0.5625\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9732 - sparse_categorical_accuracy: 0.5180 - val_loss: 0.9365 - val_sparse_categorical_accuracy: 0.5750\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9694 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.9533 - val_sparse_categorical_accuracy: 0.5406\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9687 - sparse_categorical_accuracy: 0.5416 - val_loss: 0.9540 - val_sparse_categorical_accuracy: 0.5813\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 50s 311ms/step - loss: 0.9628 - sparse_categorical_accuracy: 0.5389 - val_loss: 0.9651 - val_sparse_categorical_accuracy: 0.5516\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9761 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.9669 - val_sparse_categorical_accuracy: 0.5297\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 50s 311ms/step - loss: 0.9601 - sparse_categorical_accuracy: 0.5342 - val_loss: 0.9218 - val_sparse_categorical_accuracy: 0.5688\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9515 - sparse_categorical_accuracy: 0.5355 - val_loss: 0.9510 - val_sparse_categorical_accuracy: 0.5688\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9833 - sparse_categorical_accuracy: 0.5367 - val_loss: 0.9351 - val_sparse_categorical_accuracy: 0.5219\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9545 - sparse_categorical_accuracy: 0.5312 - val_loss: 0.9772 - val_sparse_categorical_accuracy: 0.5172\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 1.0095 - sparse_categorical_accuracy: 0.5311 - val_loss: 0.9530 - val_sparse_categorical_accuracy: 0.5578\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9601 - sparse_categorical_accuracy: 0.5420 - val_loss: 0.9328 - val_sparse_categorical_accuracy: 0.5406\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9654 - sparse_categorical_accuracy: 0.5273 - val_loss: 0.9234 - val_sparse_categorical_accuracy: 0.5688\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 1.0773 - sparse_categorical_accuracy: 0.5238 - val_loss: 0.9913 - val_sparse_categorical_accuracy: 0.5266\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 50s 310ms/step - loss: 0.9776 - sparse_categorical_accuracy: 0.5172 - val_loss: 0.9481 - val_sparse_categorical_accuracy: 0.5609\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 48s 303ms/step - loss: 0.9602 - sparse_categorical_accuracy: 0.5340 - val_loss: 0.9278 - val_sparse_categorical_accuracy: 0.5781\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9417 - sparse_categorical_accuracy: 0.5467 - val_loss: 0.9545 - val_sparse_categorical_accuracy: 0.5437\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9393 - sparse_categorical_accuracy: 0.5441 - val_loss: 0.9186 - val_sparse_categorical_accuracy: 0.5781\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 50s 312ms/step - loss: 0.9420 - sparse_categorical_accuracy: 0.5451 - val_loss: 0.9829 - val_sparse_categorical_accuracy: 0.5063\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9387 - sparse_categorical_accuracy: 0.5416 - val_loss: 0.9253 - val_sparse_categorical_accuracy: 0.5750\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9429 - sparse_categorical_accuracy: 0.5457 - val_loss: 0.9143 - val_sparse_categorical_accuracy: 0.5891\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9403 - sparse_categorical_accuracy: 0.5477 - val_loss: 0.9384 - val_sparse_categorical_accuracy: 0.5406\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9438 - sparse_categorical_accuracy: 0.5316 - val_loss: 0.8924 - val_sparse_categorical_accuracy: 0.5766\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9411 - sparse_categorical_accuracy: 0.5477 - val_loss: 0.9147 - val_sparse_categorical_accuracy: 0.5813\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9329 - sparse_categorical_accuracy: 0.5428 - val_loss: 0.9279 - val_sparse_categorical_accuracy: 0.5734\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9916 - sparse_categorical_accuracy: 0.5395 - val_loss: 0.9288 - val_sparse_categorical_accuracy: 0.5766\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9284 - sparse_categorical_accuracy: 0.5516 - val_loss: 0.9329 - val_sparse_categorical_accuracy: 0.5719\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9426 - sparse_categorical_accuracy: 0.5354 - val_loss: 0.9239 - val_sparse_categorical_accuracy: 0.5859\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 1.0026 - sparse_categorical_accuracy: 0.5408 - val_loss: 0.8986 - val_sparse_categorical_accuracy: 0.5625\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9500 - sparse_categorical_accuracy: 0.5488 - val_loss: 0.9391 - val_sparse_categorical_accuracy: 0.5734\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9346 - sparse_categorical_accuracy: 0.5500 - val_loss: 0.9265 - val_sparse_categorical_accuracy: 0.5797\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9268 - sparse_categorical_accuracy: 0.5541 - val_loss: 0.9317 - val_sparse_categorical_accuracy: 0.5625\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9291 - sparse_categorical_accuracy: 0.5502 - val_loss: 0.9167 - val_sparse_categorical_accuracy: 0.5875\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9354 - sparse_categorical_accuracy: 0.5521 - val_loss: 0.9184 - val_sparse_categorical_accuracy: 0.5703\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9232 - sparse_categorical_accuracy: 0.5564 - val_loss: 0.9331 - val_sparse_categorical_accuracy: 0.5688\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9398 - sparse_categorical_accuracy: 0.5408 - val_loss: 0.9101 - val_sparse_categorical_accuracy: 0.5922\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9362 - sparse_categorical_accuracy: 0.5480 - val_loss: 0.9388 - val_sparse_categorical_accuracy: 0.5578\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9352 - sparse_categorical_accuracy: 0.5420 - val_loss: 0.9542 - val_sparse_categorical_accuracy: 0.5594\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9427 - sparse_categorical_accuracy: 0.5424 - val_loss: 0.9021 - val_sparse_categorical_accuracy: 0.5734\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9413 - sparse_categorical_accuracy: 0.5430 - val_loss: 0.8970 - val_sparse_categorical_accuracy: 0.5766\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9383 - sparse_categorical_accuracy: 0.5416 - val_loss: 0.9123 - val_sparse_categorical_accuracy: 0.5922\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9337 - sparse_categorical_accuracy: 0.5488 - val_loss: 0.9014 - val_sparse_categorical_accuracy: 0.5906\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9366 - sparse_categorical_accuracy: 0.5447 - val_loss: 0.9250 - val_sparse_categorical_accuracy: 0.5594\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9414 - sparse_categorical_accuracy: 0.5379 - val_loss: 0.9401 - val_sparse_categorical_accuracy: 0.5453\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9319 - sparse_categorical_accuracy: 0.5488 - val_loss: 0.9349 - val_sparse_categorical_accuracy: 0.5734\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9344 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.9462 - val_sparse_categorical_accuracy: 0.5469\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9472 - sparse_categorical_accuracy: 0.5355 - val_loss: 0.9367 - val_sparse_categorical_accuracy: 0.5406\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9388 - sparse_categorical_accuracy: 0.5383 - val_loss: 0.9290 - val_sparse_categorical_accuracy: 0.5922\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9344 - sparse_categorical_accuracy: 0.5412 - val_loss: 0.9380 - val_sparse_categorical_accuracy: 0.5562\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9434 - sparse_categorical_accuracy: 0.5363 - val_loss: 0.9245 - val_sparse_categorical_accuracy: 0.5828\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9434 - sparse_categorical_accuracy: 0.5459 - val_loss: 0.9229 - val_sparse_categorical_accuracy: 0.5688\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9419 - sparse_categorical_accuracy: 0.5527 - val_loss: 0.9272 - val_sparse_categorical_accuracy: 0.5766\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9295 - sparse_categorical_accuracy: 0.5414 - val_loss: 0.9220 - val_sparse_categorical_accuracy: 0.5562\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9357 - sparse_categorical_accuracy: 0.5404 - val_loss: 0.9053 - val_sparse_categorical_accuracy: 0.5828\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9313 - sparse_categorical_accuracy: 0.5420 - val_loss: 0.9259 - val_sparse_categorical_accuracy: 0.5781\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9402 - sparse_categorical_accuracy: 0.5506 - val_loss: 0.9057 - val_sparse_categorical_accuracy: 0.5719\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9356 - sparse_categorical_accuracy: 0.5410 - val_loss: 0.9116 - val_sparse_categorical_accuracy: 0.5922\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9304 - sparse_categorical_accuracy: 0.5535 - val_loss: 0.9340 - val_sparse_categorical_accuracy: 0.5625\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9293 - sparse_categorical_accuracy: 0.5518 - val_loss: 0.9897 - val_sparse_categorical_accuracy: 0.5297\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9298 - sparse_categorical_accuracy: 0.5420 - val_loss: 0.9348 - val_sparse_categorical_accuracy: 0.5609\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9317 - sparse_categorical_accuracy: 0.5449 - val_loss: 0.9789 - val_sparse_categorical_accuracy: 0.5297\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9414 - sparse_categorical_accuracy: 0.5432 - val_loss: 0.9023 - val_sparse_categorical_accuracy: 0.5938\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9353 - sparse_categorical_accuracy: 0.5461 - val_loss: 0.9035 - val_sparse_categorical_accuracy: 0.5969\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9298 - sparse_categorical_accuracy: 0.5459 - val_loss: 0.9450 - val_sparse_categorical_accuracy: 0.5797\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9340 - sparse_categorical_accuracy: 0.5490 - val_loss: 0.9296 - val_sparse_categorical_accuracy: 0.5844\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9323 - sparse_categorical_accuracy: 0.5479 - val_loss: 0.9383 - val_sparse_categorical_accuracy: 0.5719\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9419 - sparse_categorical_accuracy: 0.5395 - val_loss: 0.9288 - val_sparse_categorical_accuracy: 0.5828\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9470 - sparse_categorical_accuracy: 0.5369 - val_loss: 0.9415 - val_sparse_categorical_accuracy: 0.5797\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9452 - sparse_categorical_accuracy: 0.5369 - val_loss: 0.9213 - val_sparse_categorical_accuracy: 0.5719\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9267 - sparse_categorical_accuracy: 0.5521 - val_loss: 0.9307 - val_sparse_categorical_accuracy: 0.5766\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9302 - sparse_categorical_accuracy: 0.5449 - val_loss: 0.9283 - val_sparse_categorical_accuracy: 0.5750\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9372 - sparse_categorical_accuracy: 0.5449 - val_loss: 0.9391 - val_sparse_categorical_accuracy: 0.5828\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9251 - sparse_categorical_accuracy: 0.5539 - val_loss: 0.9123 - val_sparse_categorical_accuracy: 0.5797\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9341 - sparse_categorical_accuracy: 0.5488 - val_loss: 0.9125 - val_sparse_categorical_accuracy: 0.5719\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9249 - sparse_categorical_accuracy: 0.5611 - val_loss: 0.8822 - val_sparse_categorical_accuracy: 0.5891\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9411 - sparse_categorical_accuracy: 0.5477 - val_loss: 0.9524 - val_sparse_categorical_accuracy: 0.5766\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9290 - sparse_categorical_accuracy: 0.5531 - val_loss: 0.9432 - val_sparse_categorical_accuracy: 0.5750\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9389 - sparse_categorical_accuracy: 0.5416 - val_loss: 1.0063 - val_sparse_categorical_accuracy: 0.4828\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9618 - sparse_categorical_accuracy: 0.5334 - val_loss: 0.9126 - val_sparse_categorical_accuracy: 0.5734\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9594 - sparse_categorical_accuracy: 0.5230 - val_loss: 0.9296 - val_sparse_categorical_accuracy: 0.5797\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9450 - sparse_categorical_accuracy: 0.5418 - val_loss: 1.0182 - val_sparse_categorical_accuracy: 0.4938\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9456 - sparse_categorical_accuracy: 0.5256 - val_loss: 0.9191 - val_sparse_categorical_accuracy: 0.5781\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9454 - sparse_categorical_accuracy: 0.5389 - val_loss: 0.9855 - val_sparse_categorical_accuracy: 0.5437\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9673 - sparse_categorical_accuracy: 0.5059 - val_loss: 1.0062 - val_sparse_categorical_accuracy: 0.4594\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9719 - sparse_categorical_accuracy: 0.5008 - val_loss: 0.9545 - val_sparse_categorical_accuracy: 0.5469\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9751 - sparse_categorical_accuracy: 0.4988 - val_loss: 0.9745 - val_sparse_categorical_accuracy: 0.5109\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9912 - sparse_categorical_accuracy: 0.4957 - val_loss: 0.9479 - val_sparse_categorical_accuracy: 0.5125\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9645 - sparse_categorical_accuracy: 0.5043 - val_loss: 0.9496 - val_sparse_categorical_accuracy: 0.5547\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9621 - sparse_categorical_accuracy: 0.5096 - val_loss: 0.9393 - val_sparse_categorical_accuracy: 0.5547\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9544 - sparse_categorical_accuracy: 0.5109 - val_loss: 0.9244 - val_sparse_categorical_accuracy: 0.5781\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9492 - sparse_categorical_accuracy: 0.5236 - val_loss: 0.9288 - val_sparse_categorical_accuracy: 0.5891\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9579 - sparse_categorical_accuracy: 0.5121 - val_loss: 0.9824 - val_sparse_categorical_accuracy: 0.5406\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 50s 315ms/step - loss: 0.9675 - sparse_categorical_accuracy: 0.5053 - val_loss: 0.9921 - val_sparse_categorical_accuracy: 0.5250\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9696 - sparse_categorical_accuracy: 0.5189 - val_loss: 1.0116 - val_sparse_categorical_accuracy: 0.4891\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9477 - sparse_categorical_accuracy: 0.5246 - val_loss: 0.9589 - val_sparse_categorical_accuracy: 0.5359\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 50s 314ms/step - loss: 0.9517 - sparse_categorical_accuracy: 0.5137 - val_loss: 0.9686 - val_sparse_categorical_accuracy: 0.5016\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 50s 313ms/step - loss: 0.9497 - sparse_categorical_accuracy: 0.5232 - val_loss: 0.9630 - val_sparse_categorical_accuracy: 0.5500\n"
     ]
    }
   ],
   "source": [
    "history = models.fit_model(\n",
    "    model,\n",
    "    train_data=train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"efficientnetv2b0-3_layer_dropout\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 4)                 11024388  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,943,700\n",
      "Trainable params: 11,024,388\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
